{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc # Note: this is a garbage collector\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"..\\Raw_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'airRes':    pd.read_csv(PATH + r\"\\air_reserve.csv\"),\n",
    "    'airStore':  pd.read_csv(PATH + r\"\\air_store_info.csv\"),\n",
    "    'airVisit':  pd.read_csv(PATH + r\"\\air_visit_data.csv\"),\n",
    "    'date':      pd.read_csv(PATH + r\"\\date_info.csv\"),\n",
    "    'hpgRes':    pd.read_csv(PATH + r\"\\hpg_reserve.csv\"),\n",
    "    'hpgStore':  pd.read_csv(PATH + r\"\\hpg_store_info.csv\"),\n",
    "    'sampleSub': pd.read_csv(PATH + r\"\\sample_submission.csv\"),\n",
    "    'storeIDs':  pd.read_csv(PATH + r\"\\store_id_relation.csv\")    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date']['visit_date'] = pd.to_datetime(data['date']['calendar_date'])\n",
    "data['date'].drop('calendar_date', axis = 1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['airVisit']['visit_date'] = pd.to_datetime(data['airVisit']['visit_date'])\n",
    "data['airVisit']['dow'] = data['airVisit']['visit_date'].dt.dayofweek\n",
    "data['airVisit']['year'] = data['airVisit']['visit_date'].dt.year\n",
    "data['airVisit']['month'] = data['airVisit']['visit_date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add EWMA of visits as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ewm (note, found this on the discussion forum):\n",
    "def calc_shifted_ewm(series, alpha, adjust = True):\n",
    "    return series.shift().ewm(alpha = alpha, adjust = adjust).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step below adds the ewm by day of week. Right now I'm returning a separate series so I can look at what each step does if I want to\n",
    "tmp = data['airVisit'].groupby(['air_store_id','dow']).apply(lambda x: calc_shifted_ewm(x['visitors'], 0.1)) \n",
    "# This step backfills the 1st week's dow for each restaurant, otherwise it would be NaN since it's a 1-period ewma\n",
    "tmp = tmp.fillna(method='bfill')\n",
    "# The groupby function returns a multiIndex Series. I only need the 3rd level (original df index) to add column to original df\n",
    "tmp.index = tmp.index.get_level_values(2)\n",
    "# Sort index before adding back to original df\n",
    "tmp = tmp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['airVisit']['ewma'] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 'days since last' and 'days until next' holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'days since holiday' feature\n",
    "daysSinceList = []\n",
    "daysSinceHol = 0 # initialize daysSince counter\n",
    "for row in data['date']['holiday_flg']:\n",
    "    if row == 1:\n",
    "        daysSinceHol = 0\n",
    "        daysSinceList.append(daysSinceHol)\n",
    "    else:\n",
    "        daysSinceHol += 1\n",
    "        daysSinceList.append(daysSinceHol)\n",
    "data['date']['days_since_holiday'] = daysSinceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'days UNTIL next holiday' feature\n",
    "holidayList = list(data['date']['holiday_flg'])\n",
    "daysUntilHolList = np.zeros(len(holidayList),dtype=np.int)\n",
    "daysUntilHol = 0 # initialize daysUntilHol counter\n",
    "for i in range(len(holidayList)-1,0,-1):\n",
    "    if holidayList[i] == 1:\n",
    "        daysUntilHol = 0\n",
    "        daysUntilHolList[i] = daysUntilHol\n",
    "    else:\n",
    "        daysUntilHol += 1\n",
    "        daysUntilHolList[i] = daysUntilHol        \n",
    "data['date']['days_until_holiday'] = daysUntilHolList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge air_visits with date dframe to get holiday info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToMerge = ['holiday_flg','visit_date','days_until_holiday','days_since_holiday']\n",
    "df_train = pd.merge(data['airVisit'], data['date'][colsToMerge], how = 'left', on = 'visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter only the stores that must be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = data['sampleSub']\n",
    "\n",
    "df_test['visit_date'] = df_test['id'].map(lambda x: str(x).split('_')[2])\n",
    "df_test['air_store_id'] = df_test['id'].map(lambda x: '_'.join(str(x).split('_')[:2]))\n",
    "df_test['visit_date'] = pd.to_datetime(df_test['visit_date'])\n",
    "df_test['dow'] = df_test['visit_date'].dt.dayofweek\n",
    "df_test['year'] = df_test['visit_date'].dt.year\n",
    "df_test['month'] = df_test['visit_date'].dt.month\n",
    "\n",
    "unique_stores = df_test['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "#stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge df_test with date dframe to get holiday info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(df_test, data['date'][colsToMerge], how = 'left', on = 'visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.merge(stores, data['airStore'], how='left', on=['air_store_id'])\n",
    "#stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical string variables \n",
    "lbl = LabelEncoder()\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add genre and area to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, stores, how = 'left', on = ['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(df_test, stores, how = 'left', on = ['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I'm just using fillna on the train set. TODO: investigate why there are NaN on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna(-1)\n",
    "df_test = df_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train\n",
    "test = df_test\n",
    "col = [c for c in train if c not in ['id', 'air_store_id','visit_date','visitors']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize XGBoost\n",
    "Note - code based on:\n",
    "https://www.kaggle.com/jmbull/no-xgb-starter-here-s-one-lb-507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binding to float32\n"
     ]
    }
   ],
   "source": [
    "# XGB starter template borrowed from @anokas\n",
    "# https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n",
    "\n",
    "print('Binding to float32')\n",
    "\n",
    "for c, dtype in zip(df_train.columns, train.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "        \n",
    "for c, dtype in zip(df_test.columns, test.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['air_store_id','visit_date','visitors'], axis=1)\n",
    "y_train = np.log1p(train['visitors'].values)\n",
    "\n",
    "# Get Column order for x_test df\n",
    "colOrder = x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252108, 11) (252108,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "#print(colOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building DMatrix...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training / validation split\n",
    "split = 200000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "\n",
    "print('Building DMatrix...')\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "del x_train, x_valid; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "[0]\ttrain-rmse:2.35565\tvalid-rmse:2.31619\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[10]\ttrain-rmse:1.61626\tvalid-rmse:1.59091\n",
      "[20]\ttrain-rmse:1.14568\tvalid-rmse:1.12901\n",
      "[30]\ttrain-rmse:0.858336\tvalid-rmse:0.847057\n",
      "[40]\ttrain-rmse:0.693608\tvalid-rmse:0.684861\n",
      "[50]\ttrain-rmse:0.605777\tvalid-rmse:0.598358\n",
      "[60]\ttrain-rmse:0.5618\tvalid-rmse:0.555074\n",
      "[70]\ttrain-rmse:0.540291\tvalid-rmse:0.534093\n",
      "[80]\ttrain-rmse:0.52968\tvalid-rmse:0.524118\n",
      "[90]\ttrain-rmse:0.524161\tvalid-rmse:0.519344\n",
      "[100]\ttrain-rmse:0.521035\tvalid-rmse:0.516998\n",
      "[110]\ttrain-rmse:0.518927\tvalid-rmse:0.51569\n",
      "[120]\ttrain-rmse:0.517322\tvalid-rmse:0.514943\n",
      "[130]\ttrain-rmse:0.515913\tvalid-rmse:0.514407\n",
      "[140]\ttrain-rmse:0.514747\tvalid-rmse:0.514137\n",
      "[150]\ttrain-rmse:0.513602\tvalid-rmse:0.514036\n",
      "[160]\ttrain-rmse:0.512643\tvalid-rmse:0.513881\n",
      "[170]\ttrain-rmse:0.511806\tvalid-rmse:0.513836\n",
      "[180]\ttrain-rmse:0.511065\tvalid-rmse:0.513836\n",
      "[190]\ttrain-rmse:0.510367\tvalid-rmse:0.513786\n",
      "[200]\ttrain-rmse:0.509749\tvalid-rmse:0.513816\n",
      "[210]\ttrain-rmse:0.509216\tvalid-rmse:0.51388\n",
      "[220]\ttrain-rmse:0.508666\tvalid-rmse:0.514001\n",
      "[230]\ttrain-rmse:0.508175\tvalid-rmse:0.514156\n",
      "[240]\ttrain-rmse:0.507715\tvalid-rmse:0.5143\n",
      "[250]\ttrain-rmse:0.507143\tvalid-rmse:0.514368\n",
      "[260]\ttrain-rmse:0.506638\tvalid-rmse:0.514473\n",
      "[270]\ttrain-rmse:0.506126\tvalid-rmse:0.514553\n",
      "[280]\ttrain-rmse:0.505648\tvalid-rmse:0.514667\n",
      "Stopping. Best iteration:\n",
      "[185]\ttrain-rmse:0.510673\tvalid-rmse:0.513771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training ...')\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'reg:linear'\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['eta'] = 0.04\n",
    "params['max_depth'] = 7\n",
    "params['silent'] = 1\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "clf = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, verbose_eval=10)\n",
    "\n",
    "del d_train, d_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign random ewma to df_test just to get a score. TODO: Figure out how to use feature in test set!\n",
    "Note: Assignment could be iterative, i.e: \n",
    "1. Assign \n",
    "2. Predict\n",
    "3. run ewma\n",
    "4. predict again\n",
    "5. run ewma, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(['id','air_store_id','visit_date','visitors'], axis=1)\n",
    "x_test['ewma'] = np.random.randint(1, 20, x_test.shape[0])\n",
    "columnsForTest_df = ['dow', 'year', 'month', 'ewma', 'holiday_flg', 'days_until_holiday',\n",
    "       'days_since_holiday', 'air_genre_name', 'air_area_name', 'latitude',\n",
    "       'longitude']\n",
    "x_test = x_test[columnsForTest_df] # Put columns in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "del x_test; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on test ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Predicting on test ...')\n",
    "\n",
    "p_test = clf.predict(d_test)\n",
    "\n",
    "del d_test; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.87995052,   8.98664856,  12.44323063, ...,   2.58029413,\n",
       "         5.66217661,  19.99444962], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expm1(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['visitors'] = np.expm1(p_test)\n",
    "\n",
    "test[['id','visitors']].to_csv('xgb_submission.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Performance wasn't very good. Predict week by week and calc moving average for next week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
